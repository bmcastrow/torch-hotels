# -*- coding: utf-8 -*-
"""HotelsClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bbruno5EYoQijbR6_rMwiMnnEFS-vU_ori-y_
    For safety reasons the url above is slightly modified (if you would like to get the google colab notebook please feel free to contact me)

# Main Goal:
- Build a neural network to predict (classify) if a customer will cancel their hotel booking reservation or not. hotels["is_canceled"] will be the binary target.

- Also do the same thing but in the context of multiclass classification. This time the target will be hotels["reservation_status]. Essentially the context here is to classify Checkouts, Cancelations and Customers that potentially will not show up "No-Shown" to the reservation.

- The dataset used is from Kaggle. You can download it in this link: https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand?resource=download

- Mini-Project done in Colab.
"""

from google.colab import files
import pandas as pd
import numpy as np

uploaded = files.upload()

hotels = pd.read_csv("hotel_bookings.csv")
hotels.head()

hotels.info()

hotels["is_canceled"].value_counts()

hotels["reservation_status"].value_counts()

hotels_groupedby = hotels.groupby("arrival_date_month")["is_canceled"].mean()
hotels_groupedby.sort_values()

"""# Data cleaning and preparation"""

object_columns = []
for col in hotels.columns:
    if hotels[col].dtype == "object" and col not in ['reservation_status_date', 'reservation_status']:
        object_columns.append(col)

print(object_columns)

hotels[object_columns].head()

hotels["hotel"].value_counts()

hotels.isnull().sum()

drop_columns = ['country', 'agent', 'company', 'reservation_status_date',
                'arrival_date_week_number', 'arrival_date_day_of_month', 'arrival_date_year'] #dropping useless columns and those that have a bunch of missing values :(

hotels = hotels.drop(labels=drop_columns, axis=1)

hotels["meal"] = hotels["meal"].replace({"Undefined": 0, "SC": 0, "BB": 1, "HB": 2, "FB": 3})
hotels["meal"]

list(hotels.columns)

object_columns

hotels.columns

one_hot_columns = ['arrival_date_month', 'distribution_channel', 'reserved_room_type', 'assigned_room_type', 'deposit_type', 'customer_type', 'market_segment']
hotels = pd.get_dummies(hotels, columns=one_hot_columns, dtype=int) #one-hot-encoding the columns
hotels.head()

hotels.columns

list(hotels.columns)#they are already one-hot-econded oopsie :]

"""# Creating the train and the test set"""

import torch
import torch.nn as nn
import torch.optim as optim

hotels = hotels.dropna()

# Remove target columns
remove_cols = ['is_canceled', 'reservation_status']

# Select training features
train_features = [x for x in hotels.columns if x not in remove_cols]

if "is_canceled" in train_features and "reservation_status" in train_features:
  print("is canceled is there")
else:
  print("not there")

(hotels[train_features])

"""# Convert to Tensors"""

hotels["is_canceled"].value_counts() #kind of an imbalanced dataset here

for col in hotels.columns:
  print(col, hotels[col].value_counts()) #just checking if theres numbers in every column and if there is no other number than 0 and 1 for certain one-hot colums

hotels[train_features].info()

hotels[train_features].isna().sum()



X = torch.tensor(hotels[train_features].values, dtype=torch.float)
y = torch.tensor(hotels['is_canceled'].values, dtype=torch.float).view(-1,1) #BCELoss expects floating.points 0.0; 1.0

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80 ,test_size=0.2, random_state=42)
print(X_train.shape)
print(X_test.shape) #shapes are good :] 73 features and almost 96k samples for the training and 24k for the test

torch.manual_seed(42)

#building a small architeture

model = nn.Sequential(
    nn.Linear(73, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 1),
    nn.Sigmoid()
)

loss = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)

"""---

## NOTES: Debugging because X_train was with Nan Values. Had to go back before splitting
"""

print("X_train NaNs:", torch.isnan(X_train).sum().item())
print("X_train Infs:", torch.isinf(X_train).sum().item())

print("y_train NaNs:", torch.isnan(y_train).sum().item())
print("y_train Infs:", torch.isinf(y_train).sum().item())

nan_mask = torch.isnan(X_train).any(dim=1)
X_train_clean = X_train[~nan_mask]
print("X_train NaNs:", torch.isnan(X_train_clean).sum().item()) #there were 4 NaNs in the X_train so i needed to drop those lines before SPLITTING!!!
#if we drop these lines and go to the training loop, what would happen is that the training would raise an error with mismatching tensors between the x train and the y_train (we droped 4 lines)

"""---"""

from sklearn.metrics import accuracy_score

num_epochs = 1000
for epoch in range(num_epochs):
    predictions = model(X_train)
    BCELoss = loss(predictions, y_train)
    BCELoss.backward()
    optimizer.step()
    optimizer.zero_grad()

    if (epoch + 1) % 100 == 0:
        predicted_labels = (predictions >= 0.5).int()
        accuracy = accuracy_score(y_train, predicted_labels)
        print(f'Epoch [{epoch+1}/{num_epochs}], 'f'BCELoss: {BCELoss.item():.4f}, 'f'Accuracy: {accuracy:.4f}') #accuracy can be misleading but i'll start with that...
        #more computational power the score would probably go up

model.eval()
with torch.no_grad():
    test_predictions = model(X_test)
    test_predicted_labels = (test_predictions >= 0.5).int()

from sklearn.metrics import accuracy_score, classification_report

test_accuracy = accuracy_score(y_test, test_predicted_labels)
print(f'Accuracy: {test_accuracy:.4f}')

report = classification_report(y_test, test_predicted_labels)
print("Classification Report:\n", report)

"""# Some conclusions

- The model is doing good, but can be better.
- Accuracy of 83%
- Precision of 85% which means that if it predicts a cancelattion it is right 85% of the times
- Recall of 67% which means that it predicts the actual cancelations  of 68%
 - Analogy with rare fish:
  - Precision: You caught 20 fish, 15 were rare, How correct are you on you say "I've caught rare fish"
  - Recall: Out of all fish in the lake, how many rares did you catch?
  Did i find ALL the rare fish or did i miss something
- Another example: 100 flights and 30 are canceled.
- High precision, Low recall:
  - system predicted 10, 9 are actual cancelations, precision 9/10
  - So 9/30 is the recall
  - System is cautious saying if thats a prediction or not
- High recall, low precision
  - System predicts 50 cancelations
  - Of those 50 only 15 are correct 15/50 is the precision
  - Out of all 30 cancelations (15/30) 50% recall
  - System makes a lot of predictions so it catches a bunch but makes false statements
- Sistema pode ser uma pessoa que omite coisas mas o que diz est√° certo
ou pode ser uma pessoa que diz muitas coisas e acaba por acertar maior parte delas mas falha muitas tambem

### Future
- For instance we can make some further in deph feature selection methods and go feature by feature in order to evaluate if it is great or not. Gathering more features....
- the neural network can be so much better optimized. More layers, optimizers, more epochs, more nodes, learning rate, regularization techniques maybe

# Multiclass classification

## Now, the reservation_status column is going to have three categories:
- Checkout label 2
- Canceled label 1
- No-Show label 0
"""

hotels["reservation_status"] = hotels["reservation_status"].replace({"Check-Out": 2, "Canceled": 1, "No-Show": 0})
hotels["reservation_status"]

Xmulti = torch.tensor(hotels[train_features].values, dtype=torch.float)
ymulti = torch.tensor(hotels['reservation_status'].values, dtype=torch.long) #CrossEntropyLoss expects integers (integers indices) to look up the correct class (in this case multiclass) thats why it is used .long

from sklearn.model_selection import train_test_split

Xmulti_train, Xmulti_test, ymulti_train, ymulti_test = train_test_split(Xmulti, ymulti,
                                                    train_size=0.8,
                                                    test_size=0.2,
                                                    random_state=42)

print("Training Shape:", Xmulti_train.shape)
print("Testing Shape:", Xmulti_test.shape)

torch.manual_seed(42)

#building a small architeture

multimodel = nn.Sequential(
    nn.Linear(73, 73),
    nn.ReLU(),
    nn.Linear(73, 36),
    nn.ReLU(),
    nn.Linear(36, 3),
    nn.Sigmoid()
)

loss = nn.CrossEntropyLoss()
optimizer = optim.Adam(multimodel.parameters(), lr=0.01)

from sklearn.metrics import accuracy_score


num_epochs = 1000
for epoch in range(num_epochs):
  predictions = multimodel(Xmulti_train)
  CELoss = loss(predictions, ymulti_train)
  CELoss.backward()
  optimizer.step()
  optimizer.zero_grad()

  if (epoch + 1) % 100 == 0:
    predicted_labels = torch.argmax(predictions, dim=1)
    accuracy = accuracy_score(ymulti_train, predicted_labels)
    print(f'Epoch [{epoch+1}/{num_epochs}], 'f'CELoss: {CELoss.item():.4f}, 'f'Accuracy: {accuracy:.4f}')

multimodel.eval()
with torch.no_grad():
    multitest_predictions = multimodel(Xmulti_test)
    multiclass_predicted_labels = torch.argmax(multitest_predictions, dim=1)

from sklearn.metrics import accuracy_score, classification_report

multiclass_accuracy = accuracy_score(y_test, multiclass_predicted_labels)
print(f'Accuracy: {multiclass_accuracy:.4f}')

multiclass_report = classification_report(y_test, multiclass_predicted_labels)
print("Classification Report:\n", multiclass_report)

"""# Some conclusions (Further development needed!)

- Accuracy seems reasonable with almost 80%. However, accuracy can be misleading in this situation where we can see of imbalanced classes.

- Class 0 has literally precision/recall 0 so the model never actually predicts class 0...

- Class 1 has a precision of 0.92 and a recall of 0.48. So when the model predicts class 1 it's 92% of the times correct but only catches half of the entire class1 . The model is not that condifent picking more examples

- Class 2 is the best. When picking class 2 the model is correct 72% of the time and the recall is 98% so he is right most of the time. really confident picking class 2 and it is actually correct.

#### Honestly high accuracy is driven by the class 2 it is the largest sample. The model ignores class 0.
#### The loss function treats every class equally. I'm using standard crossentropyloss.

# Improvements

- Collect more data for the minority class
- Do not take accuracy into consideration, macro-averaged F1 is better
- Change architeture, parameters......
"""











